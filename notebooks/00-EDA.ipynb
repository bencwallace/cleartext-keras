{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The WikiLarge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki(dataset='wikismall', data_dir='../data', keep_splits=False):\n",
    "    wiki_dir = os.path.join(data_dir, 'raw/data-simplification', dataset)\n",
    "\n",
    "    prefix = 'PWKP_108016.tag.80.aner.ori' if dataset == 'wikismall' else 'wiki.full.aner.ori'\n",
    "    data = []\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for loc in ['src', 'dst']:\n",
    "            file_name = '.'.join([prefix, split, loc])\n",
    "            file_path = os.path.join(wiki_dir, file_name)\n",
    "            stream = io.open(file_path)\n",
    "            lines = stream.read().split('\\n')\n",
    "            data.append(lines)\n",
    "\n",
    "    if keep_splits:\n",
    "        return data\n",
    "    \n",
    "    src_train, dst_train, src_valid, dst_valid, src_test, dst_test = data\n",
    "    src = src_train + src_valid + src_test\n",
    "    dst = dst_train + dst_valid + dst_test\n",
    "    return src, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dest = load_wiki(dataset='wikilarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add space around punctuation (note: punctuation can be dropped by tokenizer) and remove redundant white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = re.sub(r'([?.!])', r' \\1 ', sentence)\n",
    "    sentence = re.sub('\\s{2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return f'<START> {sentence} <END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(src[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dest = [preprocess(s) for s in src], [preprocess(s) for s in dest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually perform tokenization, limiting total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer(num_words=NUM_TOKENS, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = tokenizer.texts_to_sequences(src)\n",
    "dest_tokens = tokenizer.texts_to_sequences(dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_len = max([len(seq) for seq in src_tokens])\n",
    "max_dest_len = max([len(seq) for seq in dest_tokens])\n",
    "max_len = max([max_src_len, max_dest_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = preprocessing.sequence.pad_sequences(src_tokens, maxlen=max_len, padding='post')\n",
    "dest_tokens = preprocessing.sequence.pad_sequences(dest_tokens, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently mapping '<UNK>' to all zeros vector. See https://datascience.stackexchange.com/questions/26943/how-to-initialize-word-embeddings-for-out-of-vocabulary-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_index = dict()\n",
    "with open('../data/raw/glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        word, vec = line.split(maxsplit=1)\n",
    "        vec = np.asarray(vec.split(), dtype='float32')\n",
    "        embed_index[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embed_matrix(embed_index, tokenizer):\n",
    "    vec = next(x for x in embed_index.values())\n",
    "    dim = vec.shape[0]\n",
    "    \n",
    "    # add 1 for padding token\n",
    "    vocab_size = NUM_TOKENS + 1\n",
    "    matrix = np.zeros((vocab_size, dim))\n",
    "    for word, row in tokenizer.word_index.items():\n",
    "        if row >= vocab_size:\n",
    "            continue\n",
    "        vec = embed_index.get(word)\n",
    "        if vec is not None:\n",
    "            matrix[row] = vec\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = build_embed_matrix(embed_index, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = layers.Embedding(NUM_TOKENS + 1, 50, weights=[embed_matrix], trainable=False, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in = layers.Input(shape=(max_len,))\n",
    "enc_embed = embed(enc_in)\n",
    "_, enc_hidden, enc_cell = layers.LSTM(32, return_state=True)(enc_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note decoder internal state to be used in inference (how?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_in = layers.Input(shape=(max_len,))\n",
    "dec_embed = embed(dec_in)\n",
    "dec_out, _, _ = layers.LSTM(32, return_sequences=True, return_state=True)(dec_embed, initial_state=[enc_hidden, enc_cell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasn't there a wrapper to do this? Do we include padding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = layers.Dense(NUM_TOKENS + 1, activation='softmax')(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model([enc_in, dec_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([src_tokens, dest_tokens], dest_tokens, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
