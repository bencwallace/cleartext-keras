{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The WikiLarge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki(dataset='wikismall', data_dir='../data', keep_splits=False):\n",
    "    wiki_dir = os.path.join(data_dir, 'raw/data-simplification', dataset)\n",
    "\n",
    "    prefix = 'PWKP_108016.tag.80.aner.ori' if dataset == 'wikismall' else 'wiki.full.aner.ori'\n",
    "    data = []\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for loc in ['src', 'dst']:\n",
    "            file_name = '.'.join([prefix, split, loc])\n",
    "            file_path = os.path.join(wiki_dir, file_name)\n",
    "            stream = io.open(file_path)\n",
    "            lines = stream.read().split('\\n')\n",
    "            data.append(lines)\n",
    "\n",
    "    if keep_splits:\n",
    "        return data\n",
    "    \n",
    "    src_train, dst_train, src_valid, dst_valid, src_test, dst_test = data\n",
    "    src = src_train + src_valid + src_test\n",
    "    dst = dst_train + dst_valid + dst_test\n",
    "    return src, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dest = load_wiki(dataset='wikilarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = re.sub(r'([?.!])', r' \\1 ', sentence)\n",
    "    sentence = re.sub('\\s{2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return f'<START> {sentence} <END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> Many still refer to 25 , 50 and 75 paise as 4 , 8 and 12 annas respectively , not unlike the usage of '' bit '' in American English for Ã¢ <END>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(src[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dest = [preprocess(s) for s in src], [preprocess(s) for s in dest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178938"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=50_000, filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = tokenizer.fit_on_texts(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
